\chapter{Materiais e Métodos}\label{cap:metodologia}

Este capítulo apresenta a metodologia e os recursos utilizados no desenvolvimento deste trabalho. A \autoref{sec:config} descreve as configurações de \textit{hardware} das máquinas empregadas na execução dos experimentos. Em seguida, a \autoref{sec:benchmark} detalha as aplicações selecionadas para os testes, bem como as técnicas de otimização adotadas e o posicionamento das regiões paralelas. A \autoref{sec:desempenho} aborda os métodos utilizados para a coleta e análise dos dados de desempenho. Por fim, a \autoref{sec:resumo_exp} apresenta uma síntese de todos os experimentos realizados, organizada em formato tabular.

\section{Ambiente e Configuração Experimental}\label{sec:config}

Todos os experimentos deste trabalho foram realizados em um computador equipado com um processador Intel Core i7-4690, com frequência de 3.6 GHz, quatro núcleos físicos e suporte a oito \textit{threads}. A hierarquia de memória \textit{cache} apresenta 32 KB, 256 KB e 8 MB nos níveis L1, L2 e L3, respectivamente. O sistema possui 32 GB de memória RAM DDR3, distribuídos em quatro módulos de 8 GB configurados em \textit{dual-channel}, cada um operando a 1600 MT/s. O sistema operacional utilizado foi o Ubuntu 22.04 LTS. A \autoref{fig:compArchitecture} apresenta a hierarquia de memória e a disposição dos núcleos do processador utilizados nos testes.

\begin{figure}[htb]
	\caption{Especificação do computador utilizado em testes}
	\label{fig:compArchitecture}
	\includegraphics[scale=0.7]{figuras/architecture.pdf}
	\fonte{}
	\addcontentsline{loge}{figure}{\protect\numberline{\thefigure}Especificação do computador utilizado em testes.}
\end{figure}

Cada aplicação foi executada com diferentes quantidades de \textit{threads} (1, 2, 4 e 8), totalizando 10 execuções por configuração para reduzir variações no ambiente, todas compiladas com \textit{flag} de compilação \texttt{-O3} (nível de maior otimização do código gerado pelo compilador). Nos algoritmos que envolvem aleatoriedade, como \texttt{perfo} e \texttt{drop}, foi utilizada uma semente fixa para garantir a reprodutibilidade dos resultados. As técnicas \texttt{perfo}, \texttt{drop} e \texttt{memo} foram testadas com cinco níveis de taxa de exclusão: 0.1, 0.2, 0.3, 0.4 e 0.5.

\section{Conjunto de \textit{Benchmarks}}\label{sec:benchmark}

A seleção das aplicações que compõem o conjunto de benchmarks teve como principais critérios a tolerância a erros e a estrutura do algoritmo. Buscou-se contemplar uma gama ampla de domínios, incluindo \textit{machine learning}, álgebra linear, modelos estatísticos e multimídia. Outro objetivo foi preservar ao máximo o código original durante a inserção das regiões paralelizadas, evitando modificações significativas na implementação. Parte dessas aplicações foi obtida a partir do conjunto PolyBench~\cite{polybench}.

\subsection{2MM}\label{subsec:2mm}

2mm~\cite{polybench} é uma aplicação que realiza a multiplicação sequencial de duas matrizes, utilizando três matrizes distintas: $A$, $B$ e $C$. O processo de execução é ilustrado pelas equações \autoref{eq:dot_matrix_a} e \autoref{eq:dot_matrix_b}.

\begin{equation}
	\label{eq:dot_matrix_a}
	D = A \cdot B
\end{equation}

\begin{equation}
	\label{eq:dot_matrix_b}
	E = C \cdot D
\end{equation}

O algoritmo de multiplicação de matrizes empregado segue o padrão clássico~\cite{axler2015} descrito na \autoref{eq:multi_matrix}, que mostra a multiplicação de duas matrizes 2x2, representando o processo de somas e multiplicações sequenciais. A implementação utilizada pelo \textit{benchmark} é apresentada no \autoref{alg:mul_matrix}.

\begin{equation}
	\label{eq:multi_matrix}
	\begin{bmatrix}
		a_1 & a_2 \\
		a_3 & a_4
	\end{bmatrix}
	\cdot
	\begin{bmatrix}
		b_1 & b_2 \\
		b_3 & b_4
	\end{bmatrix}
	=
	\begin{bmatrix}
		a_1b_1 + a_2b_3 & a_1b_2 + a_2b_4 \\
		a_3b_1 + a_4b_3 & a_3b_2 + a_4b_4
	\end{bmatrix}
\end{equation}

\begin{algorithm}[htb]
	\caption{Multiplicação de matrizes}
	\label{alg:mul_matrix}
	\hrule
	\begin{algorithmic}[1]
		\REQUIRE Matrizes $A$ de tamanho $n \times m$, $B$ de tamanho $m \times p$, $C$ de tamanho $n \times p$
		\FOR{$i = 0$ até $n$, com passo $1$}
		\FOR{$j = 0$ até $p$, com passo $1$}
		\FOR{$k = 0$ até $m$, com passo $1$}
		\STATE $C[i][j] \gets C[i][j] + A[i][k] \cdot B[k][j]$
		\ENDFOR
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
	\hrule
	\fonte{\citet{polybench}}
\end{algorithm}

Nesta aplicação, ambas as multiplicações são executadas em sequência, conforme as equações \autoref{eq:dot_matrix_a} e \autoref{eq:dot_matrix_b}. Para reduzir o \textit{overhead}, optou-se por utilizar uma única região paralela que abrange ambas multiplicações. Além da paralelização foi empregado a técnica de \textit{loop tiling}~\cite{bakos2016}. Essa técnica consiste em dividir o espaço de iteração em blocos menores, \textit{tiles}, de modo que cada bloco seja completamente processado antes de avançar para o próximo. Diferentemente da iteração convencional, que percorre toda uma dimensão por vez, no \textit{tiling} os acessos a memória são reorganizados para melhorar a localidade espacial e temporal.

O principal benefício dessa otimização está no uso mais eficiente da \textit{cache}. Idealmente, cada bloco deve ser dimensionado de forma que seus dados caibam inteiramente na \textit{cache}, permitindo que todo o bloco seja carregado e processado de uma só vez. Dessa forma, reduz-se o número de acessos à memória principal e evita-se o recarregamento de dados já utilizados, aumentando o desempenho da aplicação~\cite{bakos2016}. O \autoref{alg:tiling} apresenta o pseudocódigo de implementação dessa técnica na multiplicação de matrizes.

\begin{algorithm}[htb]
	\caption{Multiplicação de matrizes com \textit{loop tiling}}
	\label{alg:tiling}
	\hrule
	\begin{algorithmic}[1]
		\REQUIRE Matrizes $A$, $B$ e $C$ de tamanho $n \times n$
		\REQUIRE Tamanho do bloco $bs$
		\FOR{$ii = 0$ até $n$, com passo $bs$}
		\FOR{$kk = 0$ até $n$, com passo $bs$}
		\FOR{$i = ii$ até $\min(ii + bs, n)$}
		\FOR{$k = kk$ até $\min(kk + bs, n)$}
		\STATE $a\_val \gets A[i][k]$
		\FOR{$j = 0$ até $n$, com passo $1$}
		\STATE $C[i][j] \gets C[i][j] + a\_val \times B[k][j]$
		\ENDFOR
		\ENDFOR
		\ENDFOR
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
	\hrule
	\fonte{}
\end{algorithm}

Outra técnica empregada, também utilizada nos demais benchmarks, foi a alocação contínua de memória. Em vez de alocar um bloco de memória separado para cada linha da matriz, foi alocado um único bloco contíguo. Esta técnica foi adotada visando á redução de chamadas de alocação e uma melhor da localidade espacial, contribuindo para um melhor desempenho.

\subsection{Correlação}\label{subsec:correlation}

A correlação é uma medida estatística que expressa o grau de associação entre duas variáveis~\cite{morettin2010}. Quando há correlação entre duas variáveis, isso indica que mudanças em uma delas tendem estar relacionadas a mudanças na outra.

A correlação pode ser positiva quando, ambas as variáveis aumentam ou diminuem juntas, ou negativa, quando uma variável aumenta enquanto a outra diminui. O valor da correlação varia de -1 a 1. Valores próximos de 1 indicam uma forte correlação positiva, enquanto valores próximos de -1, uma forte correlação negativa e valores próximos de 0 indicam uma fraca ou nenhuma correlação. A \autoref{fig:correlation}, ilustra esses três tipos de correlação, nos quais é possível observar visualmente o comportamento das variáveis em cada caso.

\begin{figure}[htb]
	\caption{Gráfico do cálculo de coeficiente de correlação de Pearson}
	\centering
	\includegraphics[scale=0.7]{figuras/correlation.png}
	\label{fig:correlation}
	\fonte{\citet{wikimediaCorrelation}}
	\addcontentsline{lof}{figure}{\protect\numberline{\thefigure}Gráfico do cálculo de coeficiente de correlação de Pearson}
\end{figure}

A fórmula comumente utilizada para calcular a correlação é chamada de \textit{Pearson Correlation Formula}, apresentada na \autoref{eq:correlation}~\cite{morettin2010}. Nessa equação, $x$ e $y$ representam os valores das variáveis, enquanto $m_x$ e $m_y$ correspondem as suas respectivas médias. A implementação utilizada pelo \textit{benchmark}~\cite{polybench} é apresentada em \autoref{alg:correlation_matrix} e \autoref{alg:correlation}, as quais são respectivamente o algoritmo para a construção da matriz de correlação e o cálculo do coeficiente em si.

\begin{equation}
	\label{eq:correlation}
	r = \frac{\sum(x - m_x)(y - m_y)}{\sqrt{\sum(x-m_x)^2 \sum(y - m_y)^2}}
\end{equation}

\begin{algorithm}[htb]
	\caption{Construção da matriz de correlação}
	\label{alg:correlation_matrix}
	\hrule
	\begin{algorithmic}[1]
		\REQUIRE Matrizes $C$ e $D$ de tamanho $n \times m$
		\FOR{$i = 0$ até $n$, com passo $1$}
		\FOR{$j = 0$ até $n$, com passo $1$}
		\IF{(j == i)}
		\STATE $r \gets 1.0$
		\ELSE
		\STATE $r \gets \texttt{correlation}(\texttt{linha}(D, i),\ \texttt{linha}(D, j),\ m)$
		\ENDIF
		\STATE $C[i][j] \gets r$
		\STATE $C[j][i] \gets r$
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
	\hrule
	\fonte{}
\end{algorithm}

\begin{algorithm}[htb]
	\caption{Cálculo do coeficiente de correlação de Pearson}
	\label{alg:correlation}
	\hrule
	\begin{algorithmic}[1]
		\REQUIRE \textit{Array} $X$ e $Y$ de tamanho $m$
		\STATE $sumX \gets 0$
		\STATE $sumY \gets 0$
		\STATE $sumXY \gets 0$
		\STATE $sumX2 \gets 0$
		\STATE $sumY2 \gets 0$
		\FOR{$i = 0$ até $n$, em passos de $1$}
		\STATE $\texttt{sumX} \gets \texttt{sumX} + X[i]$
		\STATE $\texttt{sumY} \gets \texttt{sumY} + Y[i]$
		\STATE $\texttt{sumXY} \gets \texttt{sumXY} + X[i] * Y[i]$
		\STATE $\texttt{sumX2} \gets \texttt{sumX2} + X[i] * X[i]$
		\STATE $\texttt{sumY2} \gets \texttt{sumY2} + Y[i] * Y[i]$
		\ENDFOR
	\end{algorithmic}
	\hrule
	\fonte{}
\end{algorithm}

O cálculo da correlação entre as diferentes variáveis está apresentado na \autoref{tab:correlation}. Uma importante propriedade desse cálculo, está no fato de que ela é uma matriz simétrica, isto é, os valores localizados na parte inferior da matriz são iguais aos da parte superior em relação à diagonal principal. Essa diagonal principal contém valores iguais a $1.0$, correspondentes a autocorrelação perfeita de cada variável. Essa simetria permite otimizar o cálculo, reduzindo o número de iterações, uma vez que é suficiente computar os coeficientes para somente uma das metades da matriz.

\begin{table}[htb]
	\caption{Representação da correlação entre as diferentes variáveis}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		           & \textbf{a} & \textbf{b} & \textbf{c} & \textbf{d} \\
		\hline
		\textbf{a} & 1.0        & ab         & ac         & ad         \\
		\hline
		\textbf{b} & ab         & 1.0        & bc         & bd         \\
		\hline
		\textbf{c} & ac         & bc         & 1.0        & cd         \\
		\hline
		\textbf{d} & ad         & bd         & cd         & 1.0        \\
		\hline
	\end{tabular}
	\fonte{}
	\label{tab:correlation}
\end{table}

No \autoref{alg:correlation_matrix} é possível observar que essa propriedade de simétrica foi explorada, e é também nessa região de código, mais especificamente em seu laço mais externo, que foi adicionado as anotações de paralelização, permitindo que o cálculo dos coeficientes fossem executados concorrententemente.

\subsection{Deriche}\label{subsec:deriche}

\begin{algorithm}[htb]
	\caption{Algoritmo Deriche de suavização}
	\label{alg:deriche1d}
	\hrule
	\begin{algorithmic}[1]
		\REQUIRE Imagem $I_{in}$ de tamanho $h \times w$
		\REQUIRE Coeficiente de suavização $\alpha$
		\STATE $I_{out1} \gets$ nova Matriz $h \times w$
		\STATE $I_{out2} \gets$ nova Matriz $h \times w$
		\STATE $I_{final} \gets$ nova Matriz $h \times w$

		\COMMENT{Cálculo dos coeficientes de suavização}
		\STATE $k \gets \frac{(1 - e^{-\alpha})^2}{1 + 2\alpha e^{-\alpha} - e^{-2\alpha}}$
		\STATE $a_1 \gets k$
		\STATE $a_2 \gets k \cdot e^{-\alpha} \cdot (\alpha - 1)$
		\STATE $a_3 \gets k \cdot e^{-\alpha} \cdot (\alpha + 1)$
		\STATE $a_4 \gets -k \cdot e^{-2\alpha}$
		\STATE $b_1 \gets 2 \cdot e^{-\alpha}$
		\STATE $b_2 \gets -e^{-2\alpha}$

		\COMMENT{Varredura horizontal (esquerda para direita)}
		\FOR{$i = 0$ até $h - 1$, com passo $1$}
		\STATE $y_{m1} \gets 0$, $y_{m2} \gets 0$
		\FOR{$j = 0$ até $w - 1$, com passo $1$}
		\STATE $I_{out1}[i, j] \gets a_1 \cdot I_{in}[i, j] + a_2 \cdot I_{in}[i, j-1] + b_1 \cdot y_{m1} + b_2 \cdot y_{m2}$
		\STATE $y_{m2} \gets y_{m1}$
		\STATE $y_{m1} \gets I_{out1}[i, j]$
		\ENDFOR
		\ENDFOR

		\COMMENT{Varredura horizontal (direita para esquerda)}
		\FOR{$i = 0$ até $h - 1$, com passo $1$}
		\STATE $y_{p1} \gets 0$, $y_{p2} \gets 0$
		\FOR{$j = w - 1$ até $0$, com passo $-1$}
		\STATE $I_{out2}[i, j] \gets a_3 \cdot I_{in}[i, j+1] + a_4 \cdot I_{in}[i, j+2] + b_1 \cdot y_{p1} + b_2 \cdot y_{p2}$
		\STATE $y_{p2} \gets y_{p1}$
		\STATE $y_{p1} \gets I_{out2}[i, j]$
		\ENDFOR
		\ENDFOR

		\COMMENT{Combinação final}
		\STATE $I_{final} \gets I_{out1} + I_{out2}$

		\RETURN $I_{final}$
	\end{algorithmic}
	\hrule
\end{algorithm}

Em processamento de imagens, o filtro de Deriche é um algoritmo de filtragem recursiva utilizado tanto para suavização de imagens quanto para detecção de bordas~\cite{deriche1987}. Este algoritmo foi desenvolvido como uma otimização do detector de bordas de Canny, buscando aproximar as propriedades de um filtro Gaussiano, por meio de um filtro \gls{iir}.

De forma intuitiva, o filtro de Deriche funciona de maneira diferente de um filtro baseado em uma janela fixa de pixels. Nesse algoritmo, os resultados intermediários de pixels anteriores são armazenados e são utilizados no cálculo dos pixels seguintes. O filtro percorre a imagem duas vezes, primeiramente na horizontal, da esquerda para a direita, e depois da direita para a esquerda, aplicando recursivamente os valores anteiores. Esse procecsso resulta em uma suavização eficiente, mantendo contornos suaves nas bordas da figura.

O nível dessa suavização é controlado pelo parâmetro $\alpha$, que ajusta a supressão de ruído e a precisão na localização da borda. Esse parâmetro define os coeficientes $a$ e $b$, que são utilizados tanto na \autoref{eq:deriche_esq} e \autoref{eq:deriche_dir}, que são aplicados respectivamente da esquerda para a direita e da direita para a esquerda.

\begin{equation}
	\label{eq:deriche_esq}
	y^1_{ij} = a_1x_{ij} + a_2x_{ij - 1} + b_1y^1_{ij - 1} + b_2y^1_{ij - 2}
\end{equation}

\begin{equation}
	\label{eq:deriche_dir}
	y^2_{ij} = a_3x_{ij + 1} + a_4x_{ij + 2} + b_1y^2_{ij + 1} + b_2y^2_{ij + 2}
\end{equation}

O filtro implementado segue o padrão do \textit{benchmarks}~\cite{polybench}, ou seja, apenas a suavização foi implementada, não há uma geração das bordas da imagem. O \autoref{alg:deriche}, apresenta a implementação desse algoritmo de suavização, na alteração desse \textit{benchmark} foi adicionado uma região paralela que englobava todos os laços do algoritmo.

\begin{figure}[htbp]
	\caption{Aplicação do filtro de Deriche.}
	\centering
	\subfloat[Imagem original]{
		\includegraphics[width=0.45\textwidth]{sunflower_orig.jpg}
		\label{subfig:sunflower_orig}
	}
	\hfill
	\subfloat[Imagem com Deriche aplicado]{
		\includegraphics[width=0.45\textwidth]{sunflower_deri.jpg}
		\label{subfig:sunflower_deri}
	}
	\fonte{}
	\label{fig:deriche}
\end{figure}

A aplicação desse filtro com um $\alpha = 1.5$ em uma imagem pode ser observado na \autoref{fig:deriche}, aqui temos a \autoref{subfig:sunflower_orig} como sendo a imagem original em preto e branco e a \autoref{subfig:sunflower_deri} com o filtro aplicado. É possível perceber o como o filtro suaviza as bordas das imagens, tornando mais simples o processo de detecção de bordas.

\subsection{Jacobi 2D}\label{subsec:jacobi2d}

Em análise numérica, o método de Jacobi é um algoritmo iterativo utilizado para resolver sistemas de equações lineares do tipo $Ax = b$~\cite{szep2007}. Para que o método funcione adequadamente, o sistema deve atender a algumas condições específicas:

\begin{itemize}
	\item \textbf{Matriz quadrada:} A matriz dos coeficientes deve ser quadrada, ou seja, possuir o mesmo número de linhas e colunas;
	\item \textbf{Sistema linear:} O método aplica-se exclusivamente a sistemas lineares, ou seja, não deve haver termos como multiplicação entre variáveis ($xy$), potências superiores a 1 ($x^2$, $y^3$, etc.), nem funções não lineares, como seno, cosseno ou exponenciais ($e^x$);
	\item \textbf{Diagonal dominante:} Embora não seja uma exigência absoluta, a presença de dominância diagonal na matriz dos coeficientes é altamente recomendada, pois contribui para a garantia de convergência do método. Em sua ausência, o processo iterativo pode apresentar convergência lenta ou até mesmo não convergir.
\end{itemize}

Esse algoritmo foi selecionado como parte do conjunto de \textit{benchmarks}~\cite{polybench} adotados neste trabalho por sua natureza iterativa e previsível em termos de comportamento computacional. O processo inicia-se com a atribuição de valores iniciais arbitrários ao vetor $x$. Em seguida, realiza-se uma iteração para calcular uma nova estimativa da solução, com base exclusivamente nos valores da iteração anterior. Esses novos valores são utilizados como entrada para a próxima iteração. Esse procedimento é repetido até que a diferença entre iterações consecutivas seja inferior a um limiar de tolerância pré-definido, indicando a convergência para uma solução estável.

\begin{figure}[htb]
	\caption{Execução do algoritmo de estêncil}
	\centering
	\includegraphics[scale=5]{figuras/jacobi2d.pdf}
	\label{fig:jacobi2d}
	\fonte{\citet{wikimediaJacobi2D}}
	\addcontentsline{lof}{figure}{\protect\numberline{\thefigure}Execução do algoritmo de estêncil}
\end{figure}

O Jacobi 2D é uma aplicação do método de Jacobi adaptado para problemas discretizados em duas dimensões. Sua implementação se baseia em um algoritmo de estêncil, no qual cada ponto de uma malha bidimensional é atualizado com base na média artimética dos seus quatro vizinhos imediatos (acima, abaixo, esquerda e a direita), como mostra a \autoref{fig:jacobi2d}. Após um número suficiente de iterações, a malha converge para uma configuração estável que representa uma aproximação da solução contínua do problema. O \autoref{alg:jacobi2d} apresenta a implementação utilizada pelo \textit{benchmark}.

\begin{algorithm}[htb]
	\caption{Método de Jacobi em duas dimensões}
	\label{alg:jacobi2d}
	\hrule
	\begin{algorithmic}[1]
		\REQUIRE Matriz $A$ e $B$ de tamanho $n \times n$
		\REQUIRE Quantidade de passos do algoritmo $p$
		\FOR{$t = 0$ até $p - 1$, com passo $1$}
		\FOR{$i = 1$ até $n - 1$, com passo $1$}
		\FOR{$j = 1$ até $n - 1$, com passo $1$}
		\STATE $B[i][j] \gets (A[i][j] + A[i][j + 1] + A[i][j - 1] + A[i - 1][j] + A[i + 1][j]) / 4$
		\ENDFOR
		\ENDFOR
		\FOR{$i = 1$ até $n - 1$, com passo $1$}
		\FOR{$j = 1$ até $n - 1$, com passo $1$}
		\STATE $A[i][j] \gets (B[i][j] + B[i][j + 1] + B[i][j - 1] + B[i - 1][j] + B[i+1][j]) / 4$
		\ENDFOR
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
	\hrule
	\fonte{}
\end{algorithm}

O \autoref{alg:jacobi2d} apresenta dependência sequencial entre as etapas de atualização, o que dificulta a paralelização total. Para contornar isso, a paralelização foi aplicada no laço mais externo, permitindo que cada etapa de varredura seja executada em paralelo. Já os laços internos foram anotados com diretivas de sincronização, assegurando que os dados produzidos em uma etapa estejam disponíveis antes da próxima atualização.

\subsection{K-means}\label{subsec:kmeans}

O algoritmo K-means~\cite{macqueen1967} é uma técnica de agrupamento, que tem como objetivo particionar um conjunto de dados em $k$ grupos, de forma que os dados dentro de um mesmo grupo de modo que eles sejam semelhantes entre si.

\begin{algorithm}[htb!]
	\caption{Algoritmo K-means}
	\label{alg:kmeans}
	\hrule
	\begin{algorithmic}[1]
		\REQUIRE Matriz $features$ de dimensão $numPoints$ $\times$ $numFeatures$ \\
		\REQUIRE $numClusters$, número de $clusters$ que serão formados
		\REQUIRE $iterations$, quantidade de iterações realizada pelo algoritmo
		\REQUIRE $threshold$, limiar de parada do algoritmo

		\STATE Matriz $centroids$ de tamanho $numClusters \times numFeatures$, iniciado com $0$
		\STATE Matriz $newCentroids$ de tamanho $numClusters \times numFeatures$, iniciado com $0$
		\STATE \textit{Array} $membership$ de tamanho $numPoints$, iniciado com $-1$
		\STATE \textit{Array} $newCentroidsLen$ de tamanho $numClusters$, iniciado com $0$

		\FOR{$i = 0$ até $numClusters$, com passo $1$}
		\STATE $n \gets$ índice aleatório entre $0$ e $numPoints - 1$
		\FOR{$j = 0$ até $numFeatures$, com passo $1$}
		\STATE $\textit{centroids}[i][j] \gets \textit{features}[n][j]$
		\ENDFOR
		\ENDFOR

		\FOR{$iter = 0$ até $iterations$}
		\STATE $\textit{delta} \gets 0$

		\FOR{$j = 0$ até $numPoints$}
		\STATE $index \gets \texttt{find\_nearest\_point}(centroids, features[j], numFeatures)$

		\IF{$membership[j] \neq index$}
		\STATE $delta \gets delta + 1$
		\ENDIF

		\STATE $membership[j] \gets index$
		\STATE $newCentroidsLen[index] \gets newCentroidsLen[index] + 1$

		\FOR{$k = 0$ até $numFeatures$}
		\STATE $newCentroids[index][k] \gets newCentroids[index][k] + features[j][k]$
		\ENDFOR
		\ENDFOR

		\FOR{$j = 0$ até $numClusters$}
		\IF{$newCentroidsLen[j] > 0$}
		\FOR{$k = 0$ até $numFeatures$}
		\STATE $centroids[j][k] \gets newCentroids[j][k] / newCentroidsLen[j]$
		\ENDFOR
		\ENDIF
		\ENDFOR

		\STATE Zerar $newCentroids$
		\STATE Zerar $newCentroidsLen$

		\IF{$delta \leq threshold$}
		\STATE $break$
		\ENDIF
		\ENDFOR

		\RETURN  $centroids$
	\end{algorithmic}
	\hrule
	\fonte{}
\end{algorithm}

A ideia central do algoritmo consiste em definir inicialmente $k$ centróides, que são pontos de referência no espaço de amostragem. Em seguida, cada ponto de dado é associado ao centróide mais próximo, com base em uma medida de distância, usualmente a distância euclidiana. Após essa etapa de atribuição, o algoritmo calcula a média dos pontos atribuídos a cada grupo e atualiza a posição dos centróides com base nesses valores. A \autoref{fig:kmeans} ilustra o agrupamento dos dados ao longo das iterações do algoritmo K-means. Na figura, os triângulos representam os centróides, os pontos correspondem aos dados e as regiões coloridas indicam as áreas que delimitam os agrupamentos formados.

\begin{figure}[htb]
	\caption{Agrupamentos ao longo das iterações do algoritmo K-means}
	\centering
	\includegraphics[scale=0.7]{figuras/kmeans.png}
	\label{fig:kmeans}
	\fonte{\citet{wikimediaKmeans}}
	\addcontentsline{lof}{figure}{\protect\numberline{\thefigure}Agrupamentos ao longo das iterações do algoritmo K-means}
\end{figure}

Esse processo é repetido iterativamente, os dados são reagrupados com base nos novos centróides e os centróides são novamente recalculados. O algoritmo continua até atingir um critério de parada, que pode ser um número fixo de iterações ou a estabilização dos centróides.

A \autoref{alg:kmeans}~\cite{rodinia} apresenta a implementação desse algoritmo, que reflete essa lógica de forma detalhada, incluindo a etapa de inicialização dos centróides, a atribuição de ponots, o recálculo dos centróides e a verificação do critério de parada

Durante a execução, o algoritmo também avalia a variância, uma medida que indica o quão disperso os pontos estão em relação ao seu centróide. Quanto menor essa variância, mais compactos são os grupos formados. Embora o objetivo não seja minimizar a variância até zero, uma variância excessivamente alta pode indicar que os dados não estão sendo agrupados de maneira eficiente.

Na implementação apresentada no \autoref{alg:kmeans}, a paralelização foi introduziada por meio de anotações dos laços internos, responsáveis pelas maiores cargas computacionais. Em particular, foram paralelizados os laços que percorrem os pontos de dados (\textit{numPoints}) e o laço que percorre os agrupamentos (\texttt{numclusters}).

\subsection{Mandelbrot}\label{subsec:mandelbrot}

\begin{figure}[htb]
	\caption{Representação do conjunto Mandelbrot}
	\centering
	\includegraphics[scale=0.15]{figuras/mandelbrot.png}
	\label{fig:mandelbrot}
	\fonte{}
	\addcontentsline{lof}{figure}{\protect\numberline{\thefigure}Representação do conjunto Mandelbrot}
\end{figure}

Mandelbrot é um conjunto de duas dimensções, definida no conjunto dos números complexos. Ele é construído a partir da iteração da função apresentada na \autoref{eq:mandelbrot}, onde $z$ e $c$ são números complexos, com $c$ constante para cada ponto avaliado e a condição inicial $z_0 = 0$~\cite{devaney1999}.

\begin{equation}
	\label{eq:mandelbrot}
	z_{n + 1} = z_n^{2} + c
\end{equation}

Nem todos os valores de $c$ pertencem ao conjunto de Mandelbrot. Um número complexo $c$ faz parte do conjunto se, ao aplicar a equação iterativamente, a sequência gerada não divergir, ou seja, os valores de $z_n$ permanencem no limite determinado mesmo após várias iterações. Na prática, considere-se que a sequência diverge quando $z_n > 2$, pois se isso continuará crescendo indefinidamente.

Para gerar a representação visual do conjunto Mandelbrot, mapeia-se cada pixel de uma imagem para um número complexo $c$. As regiões do plano onde $c$ pertence ao conjunto são então representadas visualmente.

Os valores de $c$ são escolhidos em um retângulo do plano complexo, com a parte real no intervalo $[-2, 1]$ e a parte imaginária em $[-1.5, 1.5]$. A conversão das coordenadas dos pixels para coordenadas no plano complexo é feita pelas equações \autoref{eq:mandelbrot_cx} e \autoref{eq:mandelbrot_cy}, onde $p_x$ e $p_y$ representam as coordenadas do pixel, e $x_{text{min}}$ e $x_{text{max}}$ definem o domínio da parte real, enquanto $y_{text{min}}$ e $y_{text{max}}$ definem o domínio da parte imaginária.

\begin{equation}
	\label{eq:mandelbrot_cx}
	c_x = x_{\text{min}} + \frac{p_x}{\text{largura da imagem}} \cdot (x_{\text{max}} - x_{\text{min}})
\end{equation}

\begin{equation}
	\label{eq:mandelbrot_cy}
	c_y = y_{\text{min}} + \frac{p_y}{\text{altura da imagem}} \cdot (y_{\text{max}} - y_{\text{min}})
\end{equation}

Inserimos os valores $c_x$ e $c_y$, nas iterações do Mandelbrot. Cada ponto da imagem é testado, e se a sequência $z_n$ não divergir após um número máximo de iterações, o pixel correspondente é colorido. A \autoref{fig:mandelbrot} apresenta uma visualização típica do conjunto de Mandelbrot. O \autoref{alg:mandelbrot}~\cite{debianBenchmarksGame} apresenta o cálculo do conjunto, e \autoref{alg:mandelbrot_image} contém o algoritmo utilizado para desenhar a imagem do conjunto.

\begin{algorithm}[htb]
	\caption{Cálculo do conjunto Mandelbrot}
	\label{alg:mandelbrot}
	\hrule
	\begin{algorithmic}[1]
		\REQUIRE Número complexo $c$
		\STATE $z \gets 0 + 0i$
		\FOR{$i = 0$ até $100$, com passo $1$}
		\STATE $z \gets z^2 + c$
		\IF{$|z| > 2.0$}
		\RETURN Falso
		\ENDIF
		\ENDFOR
		\RETURN Verdadeiro
	\end{algorithmic}
	\hrule
	\fonte{}
\end{algorithm}

\begin{algorithm}[htb]
	\caption{Geração da imagem do conjunto de Mandelbrot}
	\label{alg:mandelbrot_image}
	\hrule
	\begin{algorithmic}[1]
		\REQUIRE Tamanho da imagem base $n$
		\STATE $imageSize \gets (n / 8) \times 8$
		\STATE $pixels \gets$ vetor de tamanho $(imageSize \times imageSize) / 8$, inicializado com $0$
		\STATE $scaleX \gets (1.0 - (-2.0)) / imageSize$
		\STATE $scaleY \gets (1.5 - (-1.5)) / imageSize$

		\FOR{$pixel = 0$ até \texttt{tamanho}($pixels) - 1$}
		\STATE $byte \gets 0$
		\STATE $byteRow \gets imageSize / 8$
		\STATE $pixelColumn \gets pixel \bmod byteRow$
		\STATE $y \gets pixel / byteRow$

		\FOR{$bit = 0$ até $7$}
		\STATE $x \gets pixelColumn \times 8 + bit$
		\STATE $cx \gets -2.0 + x \times scaleX$
		\STATE $cy \gets -1.5 + y \times scaleY$
		\IF{$\texttt{mandelbrot}(cx,\ cy) =\ $Verdadeiro}
		\STATE $byte \gets byte\ |\ (1 \ll (7 - bit))$
		\ENDIF
		\ENDFOR

		\STATE $pixels[pixel] \gets byte$
		\ENDFOR
	\end{algorithmic}
	\hrule
	\fonte{}
\end{algorithm}

Na implementação apresentada, duas otimizações foram aplicadas, no \autoref{alg:mandelbrot_image}, com o objetivo de melhorar o desempenho do algoritmo. A primeira delas foi a adição de diretivas de paralelização, inseridas em seu laço mais externo, permitindo que diferentes regiões da imagem fossem processadas de forma concorrente.

A segunda otimização foi a aplicação da técnica conhecida como \textit{loop unrolling}, que consiste em transformar parte das iterações de um laço em instruções sequenciais explícitas. Essa técnica visa reduzir o overhead associado ao controle do laço, como verificações de condição e saltos, e pode beneficiar aspectos como \textit{branch prediction} e uso mais eficiente do \textit{pipeline} do processador.

\subsection{PI de Monte Carlo}\label{subsec:pi}

O método de Monte Carlo consiste em um conjunto de algoritmos estatísticos que utilizam amostragem aleatória para resolver problemas determinísticos ou estimar valores numérico~\cite{morettin2010}. Sua principal característica é a geração de entradas aleatórias dentro de um domínio previamente definido, seguido da aplicação de uma regra ou função para computar a saída, cujo resultados são então agregados para formar uma estimativa.

\begin{figure}[htb]
	\caption{Método de Monte Carlo}
	\centering
	\includegraphics[scale=0.5]{figuras/pi.png}
	\label{fig:pi}
	\fonte{\citet{wikimediaPi}}
	\addcontentsline{lof}{figure}{\protect\numberline{\thefigure}Método de Monte Carlo}
\end{figure}

No caso específico da estimativa de $\pi$, o método pode ser aplicado de forma bastante intuitiva. Considere um cículo de raio $r = 1$, inscrito em um quadrado de lado $2$, conforme ilustrado na \autoref{fig:pi}. As áreas do círculo e do quadrado são dadas, respectivamente, pela \autoref{eq:area_circle} e \autoref{eq:area_square}. A razão entre essas duas áreas é apresentado na \autoref{eq:rational_area}. Por fim ao isolarmos o $\pi$, obtemos a \autoref{eq:rational_pi}.

\begin{equation}
	\label{eq:area_circle}
	A_{\text{círculo}} = \pi r^2 = \pi
\end{equation}

\begin{equation}
	\label{eq:area_square}
	A_{\text{quadrado}} = 2 \times 2 = 4
\end{equation}

\begin{equation}
	\label{eq:rational_area}
	\frac{A_{\text{círculo}}}{A_{\text{quadrado}}} = \frac{\pi}{4}
\end{equation}

\begin{equation}
	\label{eq:rational_pi}
	\pi = 4 \times \frac{A_{\text{círculo}}}{A_{\text{quadrado}}}
\end{equation}

A partir disso, é possível aplicar o método de Monte Carlo para estimar o valor de $\pi$. Como o círculo está inscrito no quadrado, o domínio de amostragem é o próprio quadrado. Gera-se então, um grande número de pontos aleatórios dentro do quadrado. Para cada ponto $(x, y)$, calcula-se sua distância até a origem. Se a distância for menor ou igual a 1, o ponto está dentro do círculo.

A estimativa de $\pi$ é obtida pela razão entre o número de pontos que caíram dentro do círculo e o número total de pontos gerados, multiplicada por 4, como mostra a \autoref{eq:pi_monte}, a \autoref{fig:pi} apresenta uma ilustração desse processo de amostragem e da aproximação de $\pi$. O \autoref{alg:pi} apresenta a implementação dessas equações utilizada no \textit{benchmark}.

\begin{equation}
	\label{eq:pi_monte}
	\pi \approx 4 \times \frac{\text{pontos dentro do círculo}}{\text{total de pontos}}
\end{equation}

\begin{algorithm}[htb]
	\caption{Cálculo de PI pelo método de Monte Carlo}
	\label{alg:pi}
	\hrule
	\begin{algorithmic}[1]
		\REQUIRE Quantidade de iterações $n$
		\STATE $hit \gets 0$
		\STATE $tid \gets \texttt{thread\_num}()$
		\STATE $seedState \gets \{tid, tid + 1\}$
		\FOR{$i = 0$ até $n$, com passo $1$}
		\STATE $x \gets \texttt{random}(seedState)$
		\STATE $y \gets \texttt{random}(seedState)$
		\IF{$(x \times x + y \times y) \leq 1$}
		\STATE $hit \gets hit + 1$
		\ENDIF
		\ENDFOR
		\RETURN $(4.0 \times hit) / n$
	\end{algorithmic}
	\hrule
	\fonte{}
\end{algorithm}

Na implementação apresentada no \autoref{alg:pi}, o laço principal foi paralelizado de forma que cada \textit{thread} execute uma fração do espaço total de iterações. Outra otimização introduzida nessa implementação, foi na geração de números pseudoaleatórios com o algoritmo \texttt{xorshift128+}, escolhido por sua alta velocidade e boa qualidade estatística.

\section{Análise de Desempenho}\label{sec:desempenho}

Para avaliar o impacto das otimizações propostas, foram conduzidos experimentos com as aplicações 2MM (sem a otimização de \textit{tiling}) e K-Means. Os testes foram executados variando o número de \textit{threads}, sendo testado com 1, 2, 4 e 8, com o objetivo de analisar os diferentes níveis de variação nos métodos de medição.

As medições foram realizadas utilizando a ferramenta \texttt{perf}, que permite coletar métricas de baixo nível do \textit{hardware} e do sistema operacional. Entre os principais eventos monitorados, destacam-se o número total de ciclos de CPU e o tempo total de execução das aplicações. Na \autoref{subfig:2mm_cycles_avg} e na \autoref{subfig:2mm_cycles_std} apresentam, respectivamente, a média de ciclos e o coeficiente de variação da aplicação 2MM. Já a \autoref{subfig:2mm_time_avg} e a \autoref{subfig:2mm_time_std} mostram a média do tempo de execução e o respectivo coeficiente de variação. O mesmo pode ser observado em relação ao K-Means, na \autoref{subfig:kmeans_cycles_avg} e na \autoref{subfig:kmeans_cycles_std}, que ilustram os resultados para os ciclos da aplicação, enquanto a \autoref{subfig:kmeans_time_avg} e a \autoref{subfig:kmeans_time_std} apresentam as medições referentes ao tempo de execução.

\begin{figure}[htbp]
	\caption{Medição de ciclos na aplicação 2MM.}
	\centering
	\subfloat[Média de ciclos]{
		\includegraphics[width=0.45\textwidth]{2mm_cycles_avg.pdf}
		\label{subfig:2mm_cycles_avg}
	}
	\hfill
	\subfloat[Coeficiente de variação dos ciclos]{
		\includegraphics[width=0.45\textwidth]{2mm_cycles_std.pdf}
		\label{subfig:2mm_cycles_std}
	}
	\fonte{}
	\label{fig:2mm_cycles}
\end{figure}

\begin{figure}[htbp]
	\caption{Medição de ciclos na aplicação K-Means.}
	\centering
	\subfloat[Média de ciclos]{
		\includegraphics[width=0.45\textwidth]{kmeans_cycles_avg.pdf}
		\label{subfig:kmeans_cycles_avg}
	}
	\hfill
	\subfloat[Coeficiente de variação dos ciclos]{
		\includegraphics[width=0.45\textwidth]{kmeans_cycles_std.pdf}
		\label{subfig:kmeans_cycles_std}
	}
	\fonte{}
	\label{fig:kmeans_cycles}
\end{figure}

\begin{figure}[htbp]
	\caption{Medição de tempo na aplicação 2MM.}
	\centering
	\subfloat[Média de tempo]{
		\includegraphics[width=0.45\textwidth]{2mm_time_avg.pdf}
		\label{subfig:2mm_time_avg}
	}
	\hfill
	\subfloat[Coeficiente de variação do tempo]{
		\includegraphics[width=0.45\textwidth]{2mm_time_std.pdf}
		\label{subfig:2mm_time_std}
	}
	\fonte{}
	\label{fig:2mm_time}
\end{figure}

\begin{figure}[htbp]
	\caption{Medição de tempo na aplicação K-Means.}
	\centering
	\subfloat[Média de tempo]{
		\includegraphics[width=0.45\textwidth]{kmeans_time_avg.pdf}
		\label{subfig:kmeans_time_avg}
	}
	\hfill
	\subfloat[Coeficiente de variação do tempo]{
		\includegraphics[width=0.45\textwidth]{kmeans_time_std.pdf}
		\label{subfig:kmeans_time_std}
	}
	\fonte{}
	\label{fig:kmeans_time}
\end{figure}

Com base nessa análise, optou-se por utilizar o tempo de execução como métrica principal para as medições de desempenho, uma vez que apresentou maior estabilidade entre as execuções e refletiu de forma mais consistente o comportamento das aplicações.

\section{Métricas de Qualidade}\label{sec:qualidade}

Para avaliar a perda de acurácia das aplicações, foram utilizadas três métricas distintas: o \gls{smape}, \gls{mcr} e o \gls{ssim}, definidos, respectivamente, em \autoref{eq:smape}, \autoref{eq:mcr} e \autoref{eq:ssim}.

\begin{equation}
	SMAPE(A, F) = \frac{100}{n} \sum^{n}_{t=1}\frac{|F_t - A_t|}{|A_t| + |F_t|}
	\label{eq:smape}
\end{equation}

\begin{equation}
	MCR(A, F) = \frac{1}{n} \sum^{n}_{t=1} \iota (A_t \neq F_t)
	\label{eq:mcr}
\end{equation}

Em que as variáveis são definidas como:
\begin{itemize}
	\item $n$: número de elementos no \textit{array} de dados;
	\item $A_t$: $t$-ésimo elemento obtido pela execução acurada;
	\item $F_t$: $t$-ésimo elemento obtido pela execução aproximada;
	\item $\iota$: função indicadora que retorna 1 quando a condição é verdadeira e 0 caso contrário.
\end{itemize}

\begin{equation}
	SSIM(x, y) = \frac{(2 \mu_x \mu_y + c_1) \cdot (2 \sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + c_1) \cdot (\sigma_x^2 + \sigma_y^2 + c_2)}
	\label{eq:ssim}
\end{equation}

Em que as variáveis são definidas como:
\begin{itemize}
	\item $\mu_x$: média dos pixels da imagem $x$;
	\item $\mu_y$: média dos pixels da imagem $y$;
	\item $\sigma_x$: variância dos pixels da imagem $x$;
	\item $\sigma_y$: variância dos pixels da imagem $y$;
	\item $\sigma_y$: variância dos pixels da imagem $y$;
	\item $c_1$ e $c_2$: constante pequena para evitar divisão por $0$;
\end{itemize}

\section{Resumo dos \textit{Benchmarks}}\label{sec:resumo_exp}

Os \textit{benchmarks} definidos neste trabalho foram sumarizados na \autoref{tab:benchmarks}. Cada conjunto de testes teve como objetivo avaliar o desempenho e a acurácia das técnicas de aproximação implementadas.

\begin{table}[htb]
	\centering
	\caption{Resumo das aplicações utilizadas nos experimentos}
	\begin{tabular}{|l|p{3.5cm}|l|l|}
		\hline
		\textbf{Aplicação} & \textbf{Fonte}                                     & \textbf{Domínio}            & \textbf{Métrica de Qualidade} \\
		\hline
		2MM                & PolyBench~\cite{polybench}                         & Álgebra Linear              & \gls{smape}                   \\
		\hline
		Correlação         & PolyBench~\cite{polybench}                         & Probabilidade e Estatística & \gls{smape}                   \\
		\hline
		Deriche            & PolyBench~\cite{polybench}                         & Processamento de Imagem     & \gls{ssim}                    \\
		\hline
		Jacobi 2D          & PolyBench~\cite{polybench}                         & Solução Numérica            & \gls{smape}                   \\
		\hline
		K-Means            & Rodinia~\cite{rodinia}                             & Aprendizado de Máquina      & \gls{mcr}                     \\
		\hline
		Mandelbrot         & Debian Benchmarks Game~\cite{debianBenchmarksGame} & Visualização Computacional  & \gls{smape}                   \\
		\hline
		PI de Monte Carlo  & Debian Benchmarks Game~\cite{debianBenchmarksGame} & Probabilidade e Estatística & \gls{smape}                   \\
		\hline
	\end{tabular}
	\fonte{}
	\label{tab:benchmarks}
\end{table}
